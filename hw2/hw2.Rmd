Homework 2
========================================================

[Assignment Details](https://github.com/arahuja/GADS4/wiki/Regression-Assignment)

```{r initialize}
library(plyr)
library(knitr)
setwd('~/Documents/GeneralAssembly/GADS4/hw2/')
```

```{r loadAndExplore}
trainfull <- read.csv('~/Documents/GeneralAssembly/arun/GADS4/data/kaggle_salary/train.csv')
str(trainfull)
```

Taken from [kaggle site](http://www.kaggle.com/c/job-salary-prediction/data).
* ID - key of the row, should be unique
* Title - Title of the job ad
* FullDescription - body of job add. Numerics have been replaced by astericks to
remove salary info, may have removed other numbers.
* LocationRaw - Location of job
* LocationNormalized - Adzuna's interpretaion of the location
* ContractType - Full time (2978), part time(578), or blank (6444)
* ContractTime - contract (605), permanent(4132), or blank (5263)
* Company - employer name
* Category - 30 standard jobs as defined by adzuna
* SalaryRaw - text of salary from employer 
* SalaryNormalized - value of salary - *Predict this*
* SourceName - website where job was advertised

We're prdiciting SalaryNormalized, so let's check it out a little:
```{r fig.width=7, fig.height=6}
summary(trainfull$SalaryNormalized)
hist(trainfull$SalaryNormalized)
hist(log(trainfull$SalaryNormalized))

```
SalaryNormalized is pretty skewed, but log(SalaryNormalized) looks pretty
gaussian, so let's predict that instead
```{r log}
trainfull$LogSalary <- log(trainfull$SalaryNormalized)
```


### Problem 1: Split the data into training and test sets.

I'll do 90/10 train/validate. I liked Aaron's fold field that he showed in class.


```{r splitTrain}

m = nrow(trainfull) # number of rows
trainfull$fold <- sample(1:10, m, replace = T)

n=1
test <- trainfull[trainfull$fold == n,]
train <- trainfull[trainfull$fold != n,]
```

### Problem 2: Build a simple linear regression using the available categorical variables.

Before doing the regression, let's define a coupe functions to make life cleaner:

```{r MAEMSE}

mae <- function(values,predictions){
  return(mean(abs(values - predictions),na.rm=T))
}

hw2.cv <- function(form,data) {
  r.squares <- c()
  train_mae <- c()
  test_mae <- c()
  for (i in 1:max(data$fold)) {
    train <- data[data$fold != i,]
    test <- data[data$fold == i,]
    
    model <- lm(form, data = train)
    
    r.squares <- c(r.squares,summary(model)$r.squared)
    trainPredictions <- exp(predict(model,train))
    testPredictions <- exp(predict(model,test))
    
    train_mae <- c(train_mae,mae(train$SalaryNormalized,trainPredictions))
    test_mae <- c(test_mae,mae(test$SalaryNormalized,testPredictions))
    }
  cat('Mean R.Squared: ',mean(r.squares))
  cat(' Mean Training MAE: ',mean(train_mae))
  cat(' Mean Test Set MAE: ',mean(test_mae))
  return(model) 
}

reduceFactors <- function(frame,field,newname,number){
  factorsToKeep <- names(sort(summary(frame[,c(field)]),decreasing = T)[1:number])
  frame[,c(newname)] <- factor(frame[,c(field)],levels = factorsToKeep)
  return(frame)
}

```


Without some text parsing, title, full description, and LocationRaw are useless.
Company has 10% the number of values as we have jobs, and using the raw company name seems limiting. You could probably do something fun with classifying the employer by name, but that's for another time (and category probably already includes some of that information)
ContractTime and ContractType worry me since over half of the jobs have blanks for these fields, but there is some additional info for some of the listings.
Category, LocationNormalized, and SourceName look like the best places to start, even if category and LocationNormalized have Adzuna's intrepretation built in.

```{r CategoryLocSource}
form <- LogSalary ~ Category + LocationNormalized + SourceName
model <- hw2.cv(form,trainfull)
```
This regression takes too long on the 10k set, and LocationNormalized has too
many levels (there are locations in the test set that don't appear in the traing set.) Let's ditch LocationNormalized for now and go back to just Category and
just SourceName

```{r Category}
form <- LogSalary ~ Category
model <- hw2.cv(form,trainfull)
```

The category factors alone are significant, but don't explain much of the variance, hence the low R squared and the relatively high MAE for both the training and test sets. Now just SourceName:

```{r SourceName}
form <- LogSalary ~ SourceNameReduced
trainfull <- reduceFactors(trainfull,'SourceName','SourceNameReduced',20)
model <- hw2.cv(form,trainfull)
```
The test set has some SourceName that aren't in the training set, blerg.
I set those to NA and had predict ignore them, and mae() ignores NA as well.

So SourceName does marginally better than Category, but has issues with
having enough data in each category. This could improve with more data.

Let's check out ContractType and ContractTime

```{r contracts}
form <- LogSalary ~ ContractType + ContractTime
model <- hw2.cv(form,trainfull)
```

These are crap for R squared, but the coefficients are significant, so may be
worth including with category. Soooo, let's see what we have with 
category and contracttime/type

```{r categoryContracts}
form <- LogSalary ~ Category + ContractType + ContractTime
model <- hw2.cv(form,trainfull)
```
Getting better, but 10k mean absolute error is pretty crap when the mean is 30k.
Last attempt for This problem: Category, ContractType/Time, and SourceName, and we'll ignore the mismatched SourceNames for now.

```{r Cat/Source/Type/Time}
trainfull <- reduceFactors(trainfull,'Category','CategoryReduced',20)
form <- LogSalary ~ SourceNameReduced + CategoryReduced + ContractTime + ContractTime
model <- hw2.cv(form,trainfull)
```

Well, this one has the best R-Squared of the fast models, 
and the training and test MAEs
are the lowest, but they still aren't impressive.

I just re-read the instructions and realize I forgot to try interaction terms.
Here it goes

```{r interactions}
form <- LogSalary ~ SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime
model <- hw2.cv(form,trainfull)
```
The R-Squared is better, and the training MAE is as well, but the test set MAE isn't significantly better, maybe this is approaching ooverfitting? There are plenty of Category:ContractType and Category:ContractTime that are missing in the training set. This model might be best attempted with a larger training set. I'll try a larger set before submitting the final predicitons.

### Problem 3: Install DAAG
Try out cv.lm to see about improving the models

```{r daag}
library('DAAG')
model <- lm(SalaryNormalized ~ Category + ContractType + ContractTime,data = trainfull)
test <- cv.lm(trainfull,model,m=3)

```

### Problem 4: Merge Location_Tree.csv on to your dataset


I'm not sure if turning the Location Tree into a data frame was supposed to
be complicated, but I found that the rows are different lengths, so before
converting the file into a data frame I had to set the length of each row to 5.

I could image a different approach could be looping through each 
LocationNormalized, finding a matching field in the location tree and
filling in the location fields that way...

```{r buildLocation_TreeFrame}
location_tree <- read.csv('~/Documents/GeneralAssembly/arun/GADS4/data/kaggle_salary/Location_Tree.csv',stringsAsFac = F)

location_list <- strsplit(location_tree[,1],"~")

fixlength <- function(x){
  flat <- unlist(x)
  length(flat) <- 5
  return(flat)
}
location_list <- llply(location_list,fixlength)

loc_frame <- data.frame(matrix(unlist(location_list),nrow=31762,byrow=T))
colnames(loc_frame)=c('Country','Region','SubRegion','SubSubRegion','MicroRegion')
str(loc_frame)
```

Now to merge the locations and the data. It's a little complicated because 
LocationNormalized is not always the same level in the location tree.

```{r mergeframes}
mergelocation <- function(training,loc_frame){
trainmerged <- merge(training,loc_frame,by.x = "LocationNormalized",by.y='MicroRegion')
trainmerged$MicroRegion <- droplevels(trainmerged$LocationNormalized)
loc_frame <- unique(loc_frame[,1:4])

scrap <- merge(training,loc_frame,by.x = "LocationNormalized",by.y='SubSubRegion')
scrap$SubSubRegion <- droplevels(scrap$LocationNormalized)
scrap$MicroRegion <- NA
trainmerged <- rbind(trainmerged,scrap)
loc_frame <- unique(loc_frame[,1:3])

scrap <- merge(training,loc_frame,by.x = "LocationNormalized",by.y='SubRegion')
scrap$SubRegion <- droplevels(scrap$LocationNormalized)
scrap$SubSubRegion <- NA
scrap$MicroRegion <- NA
trainmerged <- rbind(trainmerged,scrap)

loc_frame <- unique(loc_frame[,1:2])
scrap <- merge(training,loc_frame,by.x = "LocationNormalized",by.y='Region')
scrap$Region <- droplevels(scrap$LocationNormalized)
scrap$SubRegion <- NA
scrap$SubSubRegion <- NA
scrap$MicroRegion <- NA
trainmerged <- rbind(trainmerged,scrap)

scrap <- training[training$LocationNormalized == 'UK',]
scrap$Country <- 'UK'
scrap$Region <- NA
scrap$SubRegion <- NA
scrap$SubSubRegion <- NA
scrap$MicroRegion <- NA
trainmerged <- rbind(trainmerged,scrap)

trainmerged <- trainmerged[!duplicated(trainmerged$Id),]
return(trainmerged)
}

merged <- mergelocation(trainfull,loc_frame)

```

Ok, here's something really annoying about location. There are labels with the same name that are in different regions, and it appears to me that you can't tell which one it is. Not sure what's best to do in this situation. I don't want to have duplicate entries, so I'm going to take unique Id values. Some of these may be misclassified, but that's just something to remember for now.

I wrapped up the merging in a function so I could use it on a larger set faster.

Now for much more exiciting models, let's use location!

```{r locationModeling}
merged <- reduceFactors(merged,'Region','RegionReduced',15)
merged <- reduceFactors(merged,'SubRegion','SubRegionReduced',15)
form <- LogSalary ~ RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime
model <- hw2.cv(form,merged)

```
Looking better! Down to below 9k MAE in both the training and the test set.

Before trying out glmnet, I was to try the next larger dataset:
```{r bigger}
trainbigger <- read.csv('~/Documents/GeneralAssembly/arun/GADS4/data/kaggle_salary/train_50k.csv')
trainbigger$LogSalary <- log(trainbigger$SalaryNormalized)
trainbigger$fold <- sample(1:10, nrow(trainbigger), replace = T)
trainbigger <- reduceFactors(trainbigger,'SourceName','SourceNameReduced',20)
trainbigger <- reduceFactors(trainbigger,'Category','CategoryReduced',20)
trainbigger <- mergelocation(trainbigger,loc_frame)
trainbigger <- reduceFactors(trainbigger,'Region','RegionReduced',15)

form <- LogSalary ~ RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime
model <- hw2.cv(form,trainbigger)
```
THis is a little awkward, the MAEs are getting larger with the larger datasets,
~9.2k for the 50k and ~9.5 for the 100k. Let's move on the glmnet and text processing.

### Problem 5: Using the GLMnet package, try glm and cv.glmnet to see if you can build a better model.

Let's try just replacing lm with glm:
```{r}
hw2.cv.glm <- function(form,data) {
  r.squares <- c()
  train_mae <- c()
  test_mae <- c()
  for (i in 1:max(data$fold)) {
    train <- data[data$fold != i,]
    test <- data[data$fold == i,]
    
    model <- glm(form, data = train)
    
    r.squares <- c(r.squares,summary(model)$r.squared)
    trainPredictions <- exp(predict(model,train))
    testPredictions <- exp(predict(model,test))
    
    train_mae <- c(train_mae,mae(train$SalaryNormalized,trainPredictions))
    test_mae <- c(test_mae,mae(test$SalaryNormalized,testPredictions))
    }
  cat(' Mean Training MAE: ',mean(train_mae))
  cat(' Mean Test Set MAE: ',mean(test_mae))
  return(model) 
}

form <- LogSalary ~ RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime
model <- hw2.cv.glm(form,merged)
```

Same result as the lm revsion. I expected it, it happened, cool.
Now let's get our regularization on.

```{r}
library(glmnet)
form <- LogSalary ~RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime

r.squares <- c()
train_mae <- c()
test_mae <- c()
  for (i in 1:max(merged$fold)) {
    train <- merged[merged$fold != i,]
    test <- merged[merged$fold == i,]
    m <- model.frame(form,train)
    mtest <- model.frame(form,test)
    x <- model.matrix(form,data = m)
    y <- matrix(m$LogSalary)
    model <- glmnet(x,y)
    
    testx <- model.matrix(form,data=mtest)
    trainPredictions <- exp(predict(model,x,s=0.0005))
    testPredictions <- exp(predict(model,testx,s=0.0005))
    
    train_mae <- c(train_mae,mae(exp(m$LogSalary),trainPredictions))
    test_mae <- c(test_mae,mae(exp(mtest$LogSalary),testPredictions))
    }
mean(train_mae)
mean(test_mae)

```
Looks pretty hot. Now to use cv.glmnet to get the best value of lambda and then try it all on a larger set.

```{r cv.glmnet}
form <- LogSalary ~ RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime

i <- 1
train <- merged[merged$fold != i,]
test <- merged[merged$fold == i,]

m <- model.frame(form,train)
x <- model.matrix(form,data = m)
y <- matrix(m$LogSalary)

model <- cv.glmnet(x,y,type.measure='mae')

mtest <- model.frame(form,test)
testx <- model.matrix(form,data=mtest)

trainPredictions <- exp(predict(model,x,s="lambda.min"))
testPredictions <- exp(predict(model,testx,s="lambda.min"))

mae(exp(m$LogSalary),trainPredictions)
mae(exp(mtest$LogSalary),testPredictions)

```

Nice, down to ~8.8k for train and test set sae.

### Problem 6: Now let's try adding some text features

```{r textFeatures}
library('tm')

src <- DataframeSource(data.frame(merged$Title))
c <- Corpus(src)
dtm <- DocumentTermMatrix(c)
dtmcols <- colnames(dtm)

analysts <- rowSums(as.matrix(dtm[,grep('analy',dtmcols,ignore.case=T)]))
engineers <- rowSums(as.matrix(dtm[,grep('engine',dtmcols,ignore.case=T)]))
software <- rowSums(as.matrix(dtm[,grep('soft',dtmcols,ignore.case=T)]))
restaurant <- rowSums(as.matrix(dtm[,grep('restau',dtmcols,ignore.case=T)]))
worker <- rowSums(as.matrix(dtm[,grep('worker',dtmcols,ignore.case=T)]))
college <- rowSums(as.matrix(dtm[,grep('colle|univers',dtmcols,ignore.case=T)]))
days <- rowSums(as.matrix(dtm[,grep('day',dtmcols,ignore.case=T)]))

text_data <- cbind(merged,analysts,engineers,software,restaurant,worker,college,days)


form <- LogSalary ~ RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime + analysts + engineers + software + restaurant + worker + college + days

form <- LogSalary ~ analysts + engineers + software + restaurant + worker + college + days

model <- hw2.cv(form,text_data)
```
That's a tad better than before, but not amazinng. Instead of guessing at title words, lets' look at common words:

```{r commonTitles}
dtmcols[colSums(as.matrix(dtm)) >400]

grabTerms <- function(dtm,cols,term) {
    return(rowSums(as.matrix(dtm[,grep(term,dtmcols,ignore.case=T)])))
  }

manage <- grabTerms(dtm,dtmcols,'manag')
senior <- grabTerms(dtm,dtmcols,'senior')
develop <- grabTerms(dtm,dtmcols,'develop')
care <- grabTerms(dtm,dtmcols,'care')
home <- grabTerms(dtm,dtmcols,'home')
assist <- grabTerms(dtm,dtmcols,'assist')

text_data <- cbind(text_data,manage,senior,assist,develop,care,home)

form <- LogSalary ~ analysts + engineers + software + restaurant + worker + college + days + manage + senior + assist + develop + care + home
model <- hw2.cv(form,text_data)

form <- LogSalary ~ RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime + analysts + engineers + software + restaurant + worker + college + days + manage + senior + assist + develop + care + home
model <- hw2.cv(form,text_data)


```
Fun. Just using the title words, you can get to ~9.6k mae.
And using the whole beast, it's ~8.1k and that's withour regularization.

Let's add in regularization, and try a larger set (need to wrap up all the massaging of the dataframe)
```{r regularizeTitles}

form <- LogSalary ~ RegionReduced + SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime + analysts + engineers + software + restaurant + worker + college + days + manage + senior + assist + develop + care + home

cv.glm.hw2 <- function(form,text_data){
  library('glmnet')
  train_mae <- c()
  test_mae <- c()
    for (i in 1:max(text_data$fold)) {
      train <- text_data[text_data$fold != i,]
      test <- text_data[text_data$fold == i,]

      m <- model.frame(form,train)
      x <- model.matrix(form,data = m)
      y <- matrix(m$LogSalary)

      model <- cv.glmnet(x,y,type.measure='mae')

      mtest <- model.frame(form,test)
      testx <- model.matrix(form,data=mtest)
 
      trainPredictions <- exp(predict(model,x,s="lambda.min"))
      testPredictions <- exp(predict(model,testx,s="lambda.min"))

      train_mae <- c(train_mae,mae(exp(m$LogSalary),trainPredictions))
      test_mae <- c(test_mae,mae(exp(mtest$LogSalary),testPredictions))
      }
  cat('mean train_mae',mean(train_mae),'mean test_mae',mean(test_mae))
  return(model)
}
model <- cv.glm.hw2(form,text_data)

```

So I'm looking at ~8.2k test mae.
To wrap up this homework, I'm going to wrap up all the massaging of the data so
I can run this on a larger dataset, and then the test set to turn in.

```{r loadSalary}
loadSalary <- function(path,loc_frame){
  library('tm')
  data <- read.csv(path)
  data$FullDescription <- NA  # reduce memory usage, since we aren't using this field
  data$LogSalary <- log(data$SalaryNormalized)
  data <- reduceFactors(data,'SourceName','SourceNameReduced',20)
  data <- reduceFactors(data,'Category','CategoryReduced',20)
  data <- mergelocation(data,loc_frame)
  data <- reduceFactors(data,'Region','RegionReduced',15)
  
  m = nrow(data) # number of rows
  data$fold <- sample(1:10, m, replace = T)
  print('test')
  src <- DataframeSource(data.frame(data$Title))
  c <- Corpus(src)
  dtm <- DocumentTermMatrix(c)
  dtmcols <- colnames(dtm)
  print('test2')
  manage <- grabTerms(dtm,dtmcols,'manag')
  senior <- grabTerms(dtm,dtmcols,'senior')
  develop <- grabTerms(dtm,dtmcols,'develop')
  care <- grabTerms(dtm,dtmcols,'care')
  home <- grabTerms(dtm,dtmcols,'home')
  assist <- grabTerms(dtm,dtmcols,'assist')
  analysts <- grabTerms(dtm,dtmcols,'analy')
  engineers <- grabTerms(dtm,dtmcols,'engine')
  software <- grabTerms(dtm,dtmcols,'soft')
  restaurant <- grabTerms(dtm,dtmcols,'restau')
  worker <- grabTerms(dtm,dtmcols,'worker')
  college <- grabTerms(dtm,dtmcols,'colle|univers')
  days <- grabTerms(dtm,dtmcols,'day')  
  
  data <- cbind(data,manage,senior,assist,develop,care,home,analysts,engineers,software,restaurant,worker,college,days)
  
  return(data)
}
path <- '~/Documents/GeneralAssembly/arun/GADS4/data/kaggle_salary/train.csv'
bigger <- loadSalary(path,loc_frame)

m <- model.frame(form,data)
x <- model.matrix(form,data = m)
y <- matrix(m$LogSalary)

model <- cv.glmnet(x,y,type.measure='mae')

form <- LogSalary ~ SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime + analysts + engineers + software + restaurant + worker + college + days + manage + senior + assist + develop + care + home


path <- '~/Documents/GeneralAssembly/arun/GADS4/data/kaggle_salary/test.csv'
data <- read.csv(path)
  data$FullDescription <- NA  # reduce memory usage, since we aren't using this field
  data <- reduceFactors(data,'SourceName','SourceNameReduced',20)
  data <- reduceFactors(data,'Category','CategoryReduced',20)
    
  m = nrow(data) # number of rows
  data$fold <- sample(1:10, m, replace = T)
  print('test')
  src <- DataframeSource(data.frame(data$Title))
  c <- Corpus(src)
  dtm <- DocumentTermMatrix(c)
  dtmcols <- colnames(dtm)
  print('test2')
  manage <- grabTerms(dtm,dtmcols,'manag')
  senior <- grabTerms(dtm,dtmcols,'senior')
  develop <- grabTerms(dtm,dtmcols,'develop')
  care <- grabTerms(dtm,dtmcols,'care')
  home <- grabTerms(dtm,dtmcols,'home')
  assist <- grabTerms(dtm,dtmcols,'assist')
  analysts <- grabTerms(dtm,dtmcols,'analy')
  engineers <- grabTerms(dtm,dtmcols,'engine')
  software <- grabTerms(dtm,dtmcols,'soft')
  restaurant <- grabTerms(dtm,dtmcols,'restau')
  worker <- grabTerms(dtm,dtmcols,'worker')
  college <- grabTerms(dtm,dtmcols,'colle|univers')
  days <- grabTerms(dtm,dtmcols,'day')  
data <- cbind(data,manage,senior,assist,develop,care,home,analysts,engineers,software,restaurant,worker,college,days)



form <-  ~SourceNameReduced + CategoryReduced + ContractTime + ContractType + CategoryReduced:ContractTime + CategoryReduced:ContractType + ContractType:ContractTime + analysts + engineers + software + restaurant + worker + college + days + manage + senior + assist + develop + care + home
mtest <- model.frame(form,data,na.action=na.pass)
testx <- model.matrix(form,data=mtest)
testPredictions <- exp(predict(model,testx,s="lambda.min"))

submission <- data.frame(Id=data$Id,
                         Salary=testPredictions)
write.csv(submission, "my_submission.csv", row.names=FALSE)

```
Blarg. Took out the location pairing because it was removing some of the test rows, and added na.action=na.pass to the model.frame() call because that was removing rows as well. My predictions still have 662 NA's which totally sucks, but at least there is something...

